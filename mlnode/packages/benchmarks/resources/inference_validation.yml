models:
  qwen3_235B_fp8:
    model: "Qwen/Qwen3-235B-A22B-Instruct-2507-FP8"
    precision: "fp8"
    dtype: "float16"
    additional_args: ["--max_model_len", "10000", "--enable-expert-parallel", "--tensor-parallel-size", "4", "--pipeline-parallel-size", "1"]
  qwen3_235B_int4:
    model: "chriswritescode/Qwen3-235B-A22B-Instruct-2507-INT4-W4A16"
    precision: "int4"
    dtype: "float16"
    additional_args: ["--max_model_len", "10000", "--enable-expert-parallel", "--tensor-parallel-size", "4", "--pipeline-parallel-size", "1"]
  qwen25_7B_fp8:
    model: "RedHatAI/Qwen2.5-7B-Instruct-quantized.w8a16"
    precision: "fp8"
    dtype: "float16"
    additional_args: []
  qwen25_7B_fp8_tp:
    model: "RedHatAI/Qwen2.5-7B-Instruct-quantized.w8a16"
    precision: "fp8"
    dtype: "float16"
    additional_args: ["--tensor-parallel-size", "1", "--pipeline-parallel-size", "1"]
  qwen25_7B_int4:
    model: "Qwen/Qwen2.5-7B-Instruct-AWQ"
    precision: "int4"
    dtype: "float16"
    additional_args: []
  qwen3_32B_fp8:
    model: "Qwen/Qwen3-32B-FP8"
    precision: "fp8"
    dtype: "float16"
    additional_args: ["--max_model_len", "10000"]
  qwen3_32B_fp8_tp:
    model: "Qwen/Qwen3-32B-FP8"
    precision: "fp8"
    dtype: "float16"
    additional_args: ["--tensor-parallel-size", "4", "--pipeline-parallel-size", "1", "--max_model_len", "10000"]
  qwen3_32B_int4:
    model: "RedHatAI/Qwen3-32B-quantized.w4a16"
    precision: "int4"
    dtype: "float16"
    additional_args: ["--max_model_len", "10000"]

servers:
  server_1_H100:
    ip: "80.188.223.202"
    node_port: "17538"
    inference_port: "17979"
    gpu: "1xH100"

  server_2_4x3090:
    ip: "161.184.141.187"
    node_port: "43446"
    inference_port: "43960"
    gpu: "4x3090"

  server_3_H100:
    ip: "45.135.56.11"
    node_port: "25304"
    inference_port: "23023"
    gpu: "1xH100"


settings:
  - inference_model: qwen25_7B_fp8
    validation_model: qwen25_7B_fp8
    server_1: server_1_H100
    server_2: server_2_4x3090
  - inference_model: qwen25_7B_int4
    validation_model: qwen25_7B_fp8
    server_1: server_1_H100
    server_2: server_3_H100


run:
  exp_name: "script_test"
  output_path: "../data/inference_results" #"output_path/exp_name" will be created and results will be saved there
  batch_size: 500 
  n_prompts: 1000
  timeout: 600 #timeout for the model to load. If the model is large, this should be increased.
  tokenizer_model_name: "unsloth/llama-3-8b-Instruct"
  request:
    max_tokens: 3000
    temperature: 0.7
    seed: 42
    top_logprobs: 4


